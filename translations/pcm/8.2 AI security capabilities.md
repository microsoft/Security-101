<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b6bb7175672298d1e2f73ba7e0006f95",
  "translation_date": "2025-11-18T18:12:26+00:00",
  "source_file": "8.2 AI security capabilities.md",
  "language_code": "pcm"
}
-->
# AI security capabilities

[![Watch the video](../../translated_images/8-2_placeholder.bc988ce5dff1726a8b6f8c00b1250865ca23d02aa5cb11fb879ed1194702c99a.pcm.png)](https://learn-video.azurefd.net/vod/player?id=e0a6f844-d884-4f76-99bd-4ce9f7f73d22)

## Wetin we get now to secure AI systems?

Right now, we get plenty tools and ways wey fit help us secure AI systems:

-   **Counterfit**: Na open-source tool wey dem design to test AI systems for security. E dey help companies check AI security risks and make sure say their algorithms strong well.
-   **Adversarial Machine Learning Tools**: Dis tools dey check how strong machine learning models go fit stand against bad attacks, so dem go fit find and fix any wahala.
-   **AI Security Toolkits**: We get open-source toolkits wey dey provide things like libraries and frameworks to help secure AI systems.
-   **Collaborative Platforms**: Companies and AI communities dey join hand to create AI-specific security tools like scanners to protect the AI supply chain.

All dis tools and methods na part of the work wey people dey do to make AI systems secure from different kinds of threats. E combine research, practical tools, and teamwork for the industry to solve the special problems wey AI technology dey bring.

## Wetin be AI red teaming? How e take different from normal security red teaming?

AI red teaming get some key differences from the normal security red teaming:

-   **Focus on AI Systems**: AI red teaming dey focus on the special wahala wey AI systems like machine learning models and data pipelines fit get, no be just normal IT infrastructure.
-   **Testing AI Behavior**: E dey test how AI systems go behave when dem see strange or unexpected inputs, to find wahala wey attackers fit use.
-   **Exploring AI Failures**: E dey look both bad and harmless failures, and e dey consider plenty different kinds of people and system failures, no be only security breaches.
-   **Prompt Injection and Content Generation**: E dey also check for wahala like prompt injection, where attackers fit make AI systems produce bad or wrong content.
-   **Ethical and Responsible AI**: E dey help make sure say AI systems dey responsible and strong enough to no misbehave when people try manipulate dem.

In general, AI red teaming na bigger work wey no just dey look for security wahala but also dey test for other kinds of problems wey dey special to AI. E dey very important to help us build AI systems wey safe by understanding and fixing the new risks wey AI fit bring.

## More things to read

 - [Microsoft AI Red Team building future of safer AI | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-96948-sayoung)
 - [Announcing Microsoftâ€™s open automation framework to red team generative AI Systems | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2024/02/22/announcing-microsofts-open-automation-framework-to-red-team-generative-ai-systems/?WT.mc_id=academic-96948-sayoung)
 - [AI Security Tools: The Open-Source Toolkit | Wiz](https://www.wiz.io/academy/ai-security-tools)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Disclaimer**:  
Dis dokyument don translate wit AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator). Even though we dey try make am accurate, abeg sabi say automated translations fit get mistake or no dey 100% correct. Di original dokyument for im native language na di main correct source. For important information, e go beta make professional human translator check am. We no go fit take blame for any misunderstanding or wrong interpretation wey fit happen because you use dis translation.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->